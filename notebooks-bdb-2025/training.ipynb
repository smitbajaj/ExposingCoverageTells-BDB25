{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1JJQMRyyP4-NEK9rp1aarKRI81AkabqBt","timestamp":1735310357736}],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Defining `ManZoneTransformer` Class"],"metadata":{"id":"FfFSg-GkAG_w"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class ManZoneTransformer(nn.Module):\n","\n","  def __init__(self, feature_len=5, model_dim=64, num_heads=2, num_layers=4, dim_feedforward=256, dropout=0.1, output_dim=2):\n","      super(ManZoneTransformer, self).__init__()\n","      self.feature_norm_layer = nn.BatchNorm1d(feature_len)\n","\n","      self.feature_embedding_layer = nn.Sequential(\n","          nn.Linear(feature_len, model_dim),\n","          nn.ReLU(),\n","          nn.LayerNorm(model_dim),\n","          nn.Dropout(dropout),\n","      )\n","\n","      transformer_encoder_layer = nn.TransformerEncoderLayer(\n","          d_model=model_dim,\n","          nhead=num_heads,\n","          dim_feedforward=dim_feedforward,\n","          dropout=dropout,\n","          batch_first=True,\n","      )\n","      self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, num_layers=num_layers)\n","\n","      self.player_pooling_layer = nn.AdaptiveAvgPool1d(1)\n","\n","      self.decoder = nn.Sequential(\n","          nn.Linear(model_dim, model_dim),\n","          nn.ReLU(),\n","          nn.Dropout(dropout),\n","          nn.Linear(model_dim, model_dim // 4),\n","          nn.ReLU(),\n","          nn.LayerNorm(model_dim // 4),\n","          nn.Linear(model_dim // 4, output_dim),\n","      )\n","\n","  def forward(self, x):\n","      # x shape: (batch_size, num_players, feature_len)\n","      x = self.feature_norm_layer(x.permute(0, 2, 1)).permute(0, 2, 1)\n","      x = self.feature_embedding_layer(x)\n","      x = self.transformer_encoder(x)\n","      x = self.player_pooling_layer(x.permute(0, 2, 1)).squeeze(-1)\n","      x = self.decoder(x)\n","      return x"],"metadata":{"id":"8v2MXTzHAISS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"Hmys_rzCAS7R"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wKELQsK4-Tb4","executionInfo":{"status":"ok","timestamp":1732207519363,"user_tz":300,"elapsed":2406,"user":{"displayName":"Poker Vision","userId":"17516040938073675309"}},"outputId":"bbcc2679-f53c-4ce0-f873-d49641d8900e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from torch.utils.data import TensorDataset, DataLoader\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from torch.optim import AdamW\n","pd.options.mode.chained_assignment = None\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","batch_size = 64\n","learning_rate = 1e-3\n","\n","# weeks 1,2,4,5,6,7,8,9 are done\n","\n","# weeks_train = [1,2,3]\n","weeks_train = [9]\n","\n","for week_eval in weeks_train:\n","\n","  # loading in data & placing into DataLoader object\n","\n","  train_features = torch.load(f\"/content/drive/MyDrive/nfl-big-data-bowl-2025/features_training_week{week_eval}preds.pt\")\n","  train_targets = torch.load(f\"/content/drive/MyDrive/nfl-big-data-bowl-2025/targets_training_week{week_eval}preds.pt\")\n","\n","  val_features = torch.load(f\"/content/drive/MyDrive/nfl-big-data-bowl-2025/features_val_week{week_eval}preds.pt\")\n","  val_targets = torch.load(f\"/content/drive/MyDrive/nfl-big-data-bowl-2025/targets_val_week{week_eval}preds.pt\")\n","\n","  # move data to device (think it needs to be consistent)\n","  train_features = train_features.to(device)\n","  train_targets = train_targets.to(device)\n","  val_features = val_features.to(device)\n","  val_targets = val_targets.to(device)\n","\n","  train_dataset = TensorDataset(train_features, train_targets)\n","  val_dataset = TensorDataset(val_features, val_targets)\n","\n","  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n","  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n","\n","  # defining ManZoneTransformer params, initializing optimizer and loss_fn\n","  model = ManZoneTransformer(\n","      feature_len=5,    # num of input features (x, y, v_x, v_y, defense)\n","      model_dim=64,     # experimented with 96 & 128... seems best\n","      num_heads=2,      # 2 seems best (but may have overfit when tried 4... may be worth iterating & increasing dropout)\n","      num_layers=4,\n","      dim_feedforward=64 * 4,\n","      dropout=0.1,      # 10% dropout to prevent overfitting... iterate as model becomes more complex (industry std is higher, i believe)\n","      output_dim=2      # man or zone classification\n","  ).to(device)\n","\n","  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","  loss_fn = nn.CrossEntropyLoss()\n","\n","  # manually placing an early stopping method... will iterate on the exact value (currently 5) but want to prevent overfitting\n","  early_stopping_patience = 5\n","  best_val_loss = float('inf')\n","  epochs_no_improve = 0\n","\n","  # -- believe industry standard is closer to 50, suggests leaving room on table to grid search over hyperparams (but lacking the compute for that)\n","  num_epochs = 30 # keeping a higher mark... ~15-20 was best in previous training but early stopping should prevent overfitting...\n","\n","  train_losses = []\n","  val_losses = []\n","  val_accuracies = []\n","\n","  print()\n","  print(f\"######################### -- WEEK {week_eval} -- TRAINING #########################\")\n","  print()\n","\n","  for epoch in range(num_epochs):\n","\n","      # training phase\n","      model.train()\n","      running_loss = 0.0\n","\n","      for features, targets in train_loader:\n","          features, targets = features.to(device), targets.to(device)\n","\n","          optimizer.zero_grad()\n","          outputs = model(features)\n","          loss = loss_fn(outputs, targets)\n","          loss.backward()\n","          optimizer.step()\n","\n","          running_loss += loss.item() * features.size(0)\n","\n","      avg_train_loss = running_loss / len(train_loader.dataset)\n","      train_losses.append(avg_train_loss)\n","\n","      # validiating phase\n","      model.eval()\n","      val_running_loss = 0.0\n","      correct = 0\n","\n","      with torch.no_grad():\n","          for val_features_batch, val_targets_batch in val_loader:\n","              val_features_batch, val_targets_batch = val_features_batch.to(device), val_targets_batch.to(device)\n","\n","              val_outputs = model(val_features_batch)\n","              val_loss = loss_fn(val_outputs, val_targets_batch)\n","\n","              val_running_loss += val_loss.item() * val_features_batch.size(0)\n","\n","              _, predicted = torch.max(val_outputs, 1)\n","              correct += (predicted == val_targets_batch).sum().item()\n","\n","      avg_val_loss = val_running_loss / len(val_loader.dataset)\n","      val_losses.append(avg_val_loss)\n","      val_accuracy = correct / len(val_loader.dataset)\n","      val_accuracies.append(val_accuracy)\n","\n","      print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n","      print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n","\n","      # adding early stopping check (effort to prevent overfitting)\n","      if avg_val_loss < best_val_loss:\n","          best_val_loss = avg_val_loss\n","          epochs_no_improve = 0\n","          # saving the best model\n","          torch.save(model.state_dict(), f\"/content/drive/MyDrive/nfl-big-data-bowl-2025/best_model_week{week_eval}.pth\")\n","\n","      else:\n","          epochs_no_improve += 1\n","          if epochs_no_improve >= early_stopping_patience:\n","              print()\n","              print(f\"Early stopping triggered. Best verision saved under 'best_model_week{week_eval}.pth'\")\n","              print()\n","              break"],"metadata":{"id":"yEKY1jz_Ge9Q"},"execution_count":null,"outputs":[]}]}