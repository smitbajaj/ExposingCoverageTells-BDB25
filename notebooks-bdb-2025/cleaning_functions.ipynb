{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2h5pNy2wTOV0mBjUOmYaO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Functions (Cleaning & Tensor Prep)"],"metadata":{"id":"HIbccDMYgcom"}},{"cell_type":"code","source":["def rotate_direction_and_orientation(df):\n","\n","  \"\"\"\n","  Rotate the direction and orientation angles so that 0° points from left to right on the field, and increasing angle goes counterclockwise\n","  This should be done BEFORE the call to make_plays_left_to_right, because that function with compensate for the flipped angles.\n","\n","  :param df: the aggregate dataframe created using the aggregate_data() method\n","\n","  :return df: the aggregate dataframe with orientation and direction angles rotated 90° clockwise\n","  \"\"\"\n","\n","  df[\"o_clean\"] = (-(df[\"o\"] - 90)) % 360\n","  df[\"dir_clean\"] = (-(df[\"dir\"] - 90)) % 360\n","\n","  return df"],"metadata":{"id":"-cVsiYLCgfDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_plays_left_to_right(df):\n","\n","  \"\"\"\n","  Flip tracking data so that all plays run from left to right. The new x, y, s, a, dis, o, and dir data\n","  will be stored in new columns with the suffix \"_clean\" even if the variables do not change from their original value.\n","\n","  :param df: the aggregate dataframe created using the aggregate_data() method\n","\n","  :return df: the aggregate dataframe with the new columns such that all plays run left to right\n","  \"\"\"\n","\n","  df[\"x_clean\"] = np.where(\n","      df[\"playDirection\"] == \"left\",\n","      120 - df[\"x\"],\n","      df[\n","          \"x\"\n","      ],  # 120 because the endzones (10 yds each) are included in the [\"x\"] values\n","  )\n","\n","  df[\"y_clean\"] = df[\"y\"]\n","  df[\"s_clean\"] = df[\"s\"]\n","  df[\"a_clean\"] = df[\"a\"]\n","  df[\"dis_clean\"] = df[\"dis\"]\n","\n","  df[\"o_clean\"] = np.where(\n","      df[\"playDirection\"] == \"left\", 180 - df[\"o_clean\"], df[\"o_clean\"]\n","  )\n","\n","  df[\"o_clean\"] = (df[\"o_clean\"] + 360) % 360  # remove negative angles\n","\n","  df[\"dir_clean\"] = np.where(\n","      df[\"playDirection\"] == \"left\", 180 - df[\"dir_clean\"], df[\"dir_clean\"]\n","  )\n","\n","  df[\"dir_clean\"] = (df[\"dir_clean\"] + 360) % 360  # remove negative angles\n","\n","  return df"],"metadata":{"id":"uyqVQgwegfFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","def calculate_velocity_components(df):\n","    \"\"\"\n","    Calculate the velocity components (v_x and v_y) for each row in the dataframe.\n","\n","    :param df: the aggregate dataframe with \"_clean\" columns created using make_plays_left_to_right()\n","\n","    :return df: the dataframe with additional columns 'v_x' and 'v_y' representing the velocity components\n","    \"\"\"\n","\n","    df[\"dir_radians\"] = np.radians(df[\"dir_clean\"])\n","\n","    df[\"v_x\"] = df[\"s_clean\"] * np.cos(df[\"dir_radians\"])\n","    df[\"v_y\"] = df[\"s_clean\"] * np.sin(df[\"dir_radians\"])\n","\n","\n","    return df"],"metadata":{"id":"Ng5_dEEYi5Wx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def label_offense_defense_coverage(presnap_df, plays_df):\n","\n","  coverage_replacements = {\n","    'Cover-3 Cloud Right': 'Cover-3',\n","    'Cover-3 Cloud Left': 'Cover-3',\n","    'Cover-3 Seam': 'Cover-3',\n","    'Cover-3 Double Cloud': 'Cover-3',\n","    'Cover-6 Right': 'Cover-6',\n","    'Cover 6-Left': 'Cover-6',\n","    'Cover-1 Double': 'Cover-1'}\n","\n","  values_to_drop = [\"Miscellaneous\", \"Bracket\", \"Prevent\", \"Red Zone\", \"Goal Line\"]\n","\n","  plays_df['pff_passCoverage'] = plays_df['pff_passCoverage'].replace(coverage_replacements)\n","\n","  plays_df = plays_df.dropna(subset=['pff_passCoverage'])\n","  plays_df = plays_df[~plays_df['pff_passCoverage'].isin(values_to_drop)]\n","\n","  coverage_mapping = {\n","      'Cover-0': 0,\n","      'Cover-1': 1,\n","      'Cover-2': 2,\n","      'Cover-3': 3,\n","      'Quarters': 4,\n","      '2-Man': 5,\n","      'Cover-6': 6\n","  }\n","\n","  merged_df = presnap_df.merge(\n","      plays_df[['gameId', 'playId', 'possessionTeam', 'defensiveTeam', 'pff_passCoverage']],\n","      on=['gameId', 'playId'],\n","      how='left'\n","  )\n","\n","  merged_df['defense'] = ((merged_df['club'] == merged_df['defensiveTeam']) & (merged_df['club'] != 'football')).astype(int)\n","\n","  merged_df['pff_passCoverage'] = merged_df['pff_passCoverage'].map(coverage_mapping)\n","  merged_df.dropna(subset=['pff_passCoverage'], inplace=True)\n","\n","  return merged_df"],"metadata":{"id":"PlGcnK2Lbwog"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def label_offense_defense_manzone(presnap_df, plays_df):\n","\n","  plays_df = plays_df.dropna(subset=['pff_manZone'])\n","\n","  coverage_mapping = {\n","      'Zone': 0,\n","      'Man': 1}\n","\n","  merged_df = presnap_df.merge(\n","      plays_df[['gameId', 'playId', 'possessionTeam', 'defensiveTeam', 'pff_manZone']],\n","      on=['gameId', 'playId'],\n","      how='left'\n","  )\n","\n","  merged_df['defense'] = ((merged_df['club'] == merged_df['defensiveTeam']) & (merged_df['club'] != 'football')).astype(int)\n","\n","  merged_df['pff_manZone'] = merged_df['pff_manZone'].map(coverage_mapping)\n","  merged_df.dropna(subset=['pff_manZone'], inplace=True)\n","\n","  return merged_df"],"metadata":{"id":"9uSd9LmShIRP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def label_offense_defense_formation(presnap_df, plays_df):\n","\n","  \"\"\"\n","  Adds 'offense' and 'defense' columns to presnap_df, marking players as offense (1) or defense (0)\n","  based on possession team and defensive team from plays_df. Enumerates offensive formations\n","  and removes rows with missing formations.\n","\n","  Parameters:\n","  presnap_df (pd.DataFrame): DataFrame containing tracking data with 'gameId', 'playId', and 'club'.\n","  plays_df (pd.DataFrame): DataFrame containing 'gameId', 'playId', 'possessionTeam', 'defensiveTeam', 'offenseFormation'.\n","\n","  Returns:\n","  pd.DataFrame: Updated presnap_df with added 'offense', 'defense', and enumerated 'offenseFormation' columns, with NaN formations dropped.\n","  \"\"\"\n","\n","  formation_mapping = {\n","      'EMPTY': 0,\n","      'I_FORM': 1,\n","      'JUMBO': 2,\n","      'PISTOL': 3,\n","      'SHOTGUN': 4,\n","      'SINGLEBACK': 5,\n","      'WILDCAT': 6\n","  }\n","\n","  merged_df = presnap_df.merge(\n","      plays_df[['gameId', 'playId', 'possessionTeam', 'defensiveTeam', 'offenseFormation']],\n","      on=['gameId', 'playId'],\n","      how='left'\n","  )\n","\n","  merged_df['defense'] = ((merged_df['club'] == merged_df['defensiveTeam']) & (merged_df['club'] != 'football')).astype(int)\n","\n","  merged_df['offenseFormation'] = merged_df['offenseFormation'].map(formation_mapping)\n","  merged_df.dropna(subset=['offenseFormation'], inplace=True)\n","\n","  return merged_df"],"metadata":{"id":"hSV6VKPNgfHm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","def split_data_by_uniqueId(df, train_ratio=0.7, test_ratio=0.15, val_ratio=0.15, unique_id_column=\"uniqueId\"):\n","\n","  \"\"\"\n","  Split the dataframe into training, testing, and validation sets based on a given ratio while\n","  ensuring all rows with the same uniqueId are in the same set.\n","\n","  :param df: the aggregate dataframe containing all frames for each play\n","  :param train_ratio: proportion of the data to allocate to training (default 0.7)\n","  :param test_ratio: proportion of the data to allocate to testing (default 0.15)\n","  :param val_ratio: proportion of the data to allocate to validation (default 0.15)\n","  :param unique_id_column: the name of the column containing the unique identifiers for each play\n","\n","  :return: three dataframes (train_df, test_df, val_df) for training, testing, and validation\n","  \"\"\"\n","\n","  unique_ids = df[unique_id_column].unique()\n","  np.random.shuffle(unique_ids)\n","\n","  num_ids = len(unique_ids)\n","  train_end = int(train_ratio * num_ids)\n","  test_end = train_end + int(test_ratio * num_ids)\n","\n","  train_ids = unique_ids[:train_end]\n","  test_ids = unique_ids[train_end:test_end]\n","  val_ids = unique_ids[test_end:]\n","\n","  train_df = df[df[unique_id_column].isin(train_ids)]\n","  test_df = df[df[unique_id_column].isin(test_ids)]\n","  val_df = df[df[unique_id_column].isin(val_ids)]\n","\n","  print(f\"Train Dataframe Frames: {train_df.shape[0]}\")\n","  print(f\"Test Dataframe Frames: {test_df.shape[0]}\")\n","  print(f\"Val Dataframe Frames: {val_df.shape[0]}\")\n","\n","  return train_df, test_df, val_df"],"metadata":{"id":"7lm6zi6Vlnj9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pass_attempt_merging(tracking, plays):\n","\n","  plays['passAttempt'] = np.where(plays['passResult'].isin([np.nan, 'S']), 0, 1)\n","\n","  plays_for_merge = plays[['gameId', 'playId', 'passAttempt']]\n","\n","  merged_df = tracking.merge(\n","      plays_for_merge,\n","      on=['gameId', 'playId'],\n","      how='left')\n","\n","  return merged_df"],"metadata":{"id":"0vNB3aWd8vaY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_frame_data(df, features, target_column):\n","\n","  features_array = df.groupby(\"frameUniqueId\")[features].apply(\n","      lambda x: x.to_numpy(dtype=np.float32)).to_numpy()\n","\n","  try:\n","      features_tensor = torch.tensor(np.stack(features_array))\n","  except ValueError as e:\n","      print(\"Skipping batch due to inconsistent shapes in features_array:\", e)\n","      return None, None  # or return some placeholder values if needed\n","\n","  targets_array = df.groupby(\"frameUniqueId\")[target_column].first().to_numpy()\n","  targets_tensor = torch.tensor(targets_array, dtype=torch.long)\n","\n","  return features_tensor, targets_tensor"],"metadata":{"id":"qq-gIncr50PC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def select_augmented_frames(df, num_samples, sigma=5):\n","\n","    df_frames = df[['frameUniqueId', 'frames_from_snap']].drop_duplicates()\n","    weights = np.exp(-((df_frames['frames_from_snap'] + 10) ** 2) / (2 * sigma ** 2))\n","\n","    weights /= weights.sum()\n","\n","    selected_frames = np.random.choice(\n","        df_frames['frameUniqueId'], size=num_samples, replace=False, p=weights\n","    )\n","\n","    return selected_frames"],"metadata":{"id":"j2vwiIwt2g8p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def data_augmentation(df, augmented_frames):\n","\n","  df_sample = df.loc[df['frameUniqueId'].isin(augmented_frames)].copy()\n","\n","  df_sample['y_clean'] = (160 / 3) - df_sample['y_clean']\n","  df_sample['dir_radians'] = (2 * np.pi) - df_sample['dir_radians']\n","  df_sample['dir_clean'] = np.degrees(df_sample['dir_radians'])\n","\n","  df_sample['frameUniqueId'] = df_sample['frameUniqueId'].astype(str) + '_aug'\n","\n","  return df_sample"],"metadata":{"id":"mv4qJICHzCx1"},"execution_count":null,"outputs":[]}]}